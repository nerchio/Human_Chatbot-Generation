# The Evaluation Results
The ChatGPT4o model is used to be the evaluator.

## Uni Eval

### Passing Rate (No AI involved)
Read-world Human-Chatbot Conversations: 96.61%
llama_3B: 92.74%
llama_70B: 99.27%
GPT4oMini: 99.03%
GPT4o: 98.06%
DeepSeek: 100%


### Evaluation Cost
Read-world Human-Chatbot Conversations: 1.17$
llama_3B: 1.08$
llama_70B: 1.50$
GPT4oMini: 1.12$
GPT4o: 1.27$
DeepSeek: 1.30%

## Pair Eval

### GPT4o VS GPT4oMini
GPT4o Better: 0.24%
GPT4oMini Better: 0.48%
Both Falied the Test: 0.73%
Both Pass the Test: 98.55%

Cost: 2.10$

### GPT4o VS DeepSeek
GPT4o Better: 0.24%
DeepSeek Better: 0.24%
Both Falied the Test: 0%
Both Pass the Test: 99.52%

Cost: 2.28$

### GPT4o VS llama_70B
GPT4o Better: 0.48%
DeepSeek Better: 0.24%
Both Falied the Test: 0.24%
Both Pass the Test: 99.03%

Cost: 2.48$

### llama_70B VS llama_30B
GPT4o Better: 8.23%
DeepSeek Better: 2.66%
Both Falied the Test: 0.48%
Both Pass the Test: 88.62%

Cost: 2.28$